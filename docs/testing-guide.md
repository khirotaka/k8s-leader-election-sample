# Leader Election ãƒ†ã‚¹ãƒˆå®Œå…¨ã‚¬ã‚¤ãƒ‰

> **å¯¾è±¡èª­è€…**: Kubernetesã®åŸºæœ¬çš„ãªæ“ä½œã¯ç†è§£ã—ã¦ã„ã‚‹ãŒã€Leader
> Electionã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚¹ãƒˆæ–¹æ³•ã‚’å­¦ã³ãŸã„Goã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢

## ğŸ“š ç›®æ¬¡

1. [ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®æ¦‚è¦](#1-ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®æ¦‚è¦)
2. [ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ](#2-ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ)
3. [çµ±åˆãƒ†ã‚¹ãƒˆ](#3-çµ±åˆãƒ†ã‚¹ãƒˆ)
4. [E2Eãƒ†ã‚¹ãƒˆï¼ˆGinkgoï¼‰](#4-e2eãƒ†ã‚¹ãƒˆginkgo)
5. [GitHub Actions ã§ã®CI/CDè¨­å®š](#5-github-actions-ã§ã®cicdè¨­å®š)
6. [ãƒ†ã‚¹ãƒˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](#6-ãƒ†ã‚¹ãƒˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹)

---

## 1. ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®æ¦‚è¦

### 1.1 ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰

Leader Electionã®ãƒ†ã‚¹ãƒˆã¯ã€ä»¥ä¸‹ã®3å±¤ã§æ§‹æˆã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰"
        E2E["ğŸ”º E2Eãƒ†ã‚¹ãƒˆ<br/>å°‘æ•°ãƒ»é«˜ã‚³ã‚¹ãƒˆãƒ»é«˜ä¿¡é ¼æ€§"]
        INT["ğŸ”¶ çµ±åˆãƒ†ã‚¹ãƒˆ<br/>ä¸­ç¨‹åº¦"]
        UNIT["ğŸŸ¢ ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ<br/>å¤šæ•°ãƒ»ä½ã‚³ã‚¹ãƒˆãƒ»é«˜é€Ÿ"]
    end
    
    E2E --> INT
    INT --> UNIT
    
    style E2E fill:#ff6b6b,color:#fff
    style INT fill:#ffd93d,color:#000
    style UNIT fill:#6bcb77,color:#fff
```

### 1.2 å„ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´

| ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«       | å®Ÿè¡Œé€Ÿåº¦          | K8sã‚¯ãƒ©ã‚¹ã‚¿ |   CIã§ã®å®Ÿè¡Œ    | ã‚«ãƒãƒ¼ç¯„å›²     |
| ------------------ | ----------------- | :---------: | :-------------: | -------------- |
| **ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ** | âš¡ é«˜é€Ÿï¼ˆæ•°ç§’ï¼‰   |   âŒ ä¸è¦   |     âœ… å®¹æ˜“     | ãƒ­ã‚¸ãƒƒã‚¯ã®æ¤œè¨¼ |
| **çµ±åˆãƒ†ã‚¹ãƒˆ**     | ğŸš€ ä¸­é€Ÿï¼ˆæ•°åˆ†ï¼‰   |   âœ… å¿…è¦   | âœ… å¯èƒ½ï¼ˆKindï¼‰ | APIã¨ã®é€£æº    |
| **E2Eãƒ†ã‚¹ãƒˆ**      | ğŸ¢ ä½é€Ÿï¼ˆæ•°åˆ†ã€œï¼‰ |   âœ… å¿…è¦   | âœ… å¯èƒ½ï¼ˆKindï¼‰ | ã‚·ãƒŠãƒªã‚ªå…¨ä½“   |

### 1.3 ãƒ†ã‚¹ãƒˆç’°å¢ƒã®é¸æŠ

```mermaid
flowchart TD
    START[ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚’æ±ºå®š] --> Q1{K8s APIã¨ã®<br/>é€£æºãŒå¿…è¦?}
    Q1 -->|No| UNIT[ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ<br/>fake clientä½¿ç”¨]
    Q1 -->|Yes| Q2{å®Ÿéš›ã®Podå‹•ä½œ<br/>ã‚’æ¤œè¨¼?}
    Q2 -->|No| INT[çµ±åˆãƒ†ã‚¹ãƒˆ<br/>Kind + Go test]
    Q2 -->|Yes| E2E[E2Eãƒ†ã‚¹ãƒˆ<br/>Kind + Ginkgo]
    
    UNIT --> CI_SIMPLE[GitHub Actions<br/>æ¨™æº–ãƒ©ãƒ³ãƒŠãƒ¼]
    INT --> CI_KIND[GitHub Actions<br/>Kind ã‚¯ãƒ©ã‚¹ã‚¿]
    E2E --> CI_KIND
    
    style UNIT fill:#6bcb77
    style INT fill:#ffd93d
    style E2E fill:#ff6b6b,color:#fff
```

---

## 2. ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ

### 2.1 æ¦‚è¦

ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã§ã¯ã€`client-go` ã® **fake client**
ã‚’ä½¿ç”¨ã—ã¦ã€å®Ÿéš›ã®Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ãªã—ã§Leader
Electionã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

```mermaid
graph LR
    subgraph "ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆç’°å¢ƒ"
        TEST[ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰] --> FAKE[Fake Clientset]
        FAKE --> MEM[(ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒª<br/>ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆã‚¢)]
    end
    
    subgraph "æœ¬ç•ªç’°å¢ƒ"
        APP[ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³] --> REAL[Real Clientset]
        REAL --> API[API Server]
        API --> ETCD[(etcd)]
    end
    
    style FAKE fill:#6bcb77,color:#fff
    style REAL fill:#4a90d9,color:#fff
```

### 2.2 å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```go
import (
    "context"
    "testing"
    "time"

    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes/fake"
    "k8s.io/client-go/tools/leaderelection"
    "k8s.io/client-go/tools/leaderelection/resourcelock"
)
```

### 2.3 åŸºæœ¬çš„ãªãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³

#### 2.3.1 ãƒªãƒ¼ãƒ€ãƒ¼é¸å‡ºã®æˆåŠŸãƒ†ã‚¹ãƒˆ

```go
// main_test.go
package main

import (
    "context"
    "sync"
    "testing"
    "time"

    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes/fake"
    "k8s.io/client-go/tools/leaderelection"
    "k8s.io/client-go/tools/leaderelection/resourcelock"
)

func TestLeaderElection_SingleCandidate(t *testing.T) {
    // Arrange: Fake clientset ã‚’ä½œæˆ
    clientset := fake.NewSimpleClientset()

    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()

    lock := &resourcelock.LeaseLock{
        LeaseMeta: metav1.ObjectMeta{
            Name:      "test-lease",
            Namespace: "default",
        },
        Client: clientset.CoordinationV1(),
        LockConfig: resourcelock.ResourceLockConfig{
            Identity: "test-pod-1",
        },
    }

    // Act: ãƒªãƒ¼ãƒ€ãƒ¼é¸å‡ºã‚’å®Ÿè¡Œ
    leaderElected := make(chan struct{})
    var wg sync.WaitGroup
    wg.Add(1)

    go func() {
        defer wg.Done()
        leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
            Lock:            lock,
            ReleaseOnCancel: true,
            LeaseDuration:   5 * time.Second,
            RenewDeadline:   3 * time.Second,
            RetryPeriod:     1 * time.Second,
            Callbacks: leaderelection.LeaderCallbacks{
                OnStartedLeading: func(ctx context.Context) {
                    close(leaderElected)
                    // ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã—ã¦ã®å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    <-ctx.Done()
                },
                OnStoppedLeading: func() {
                    t.Log("Leadership lost")
                },
                OnNewLeader: func(identity string) {
                    t.Logf("New leader: %s", identity)
                },
            },
        })
    }()

    // Assert: ãƒªãƒ¼ãƒ€ãƒ¼ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèª
    select {
    case <-leaderElected:
        t.Log("âœ… Successfully became leader")
    case <-time.After(10 * time.Second):
        t.Fatal("âŒ Timeout waiting to become leader")
    }

    // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cancel()
    wg.Wait()
}
```

#### 2.3.2 è¤‡æ•°å€™è£œè€…ã§ã®ç«¶åˆãƒ†ã‚¹ãƒˆ

```go
func TestLeaderElection_MultipleCandidates(t *testing.T) {
    clientset := fake.NewSimpleClientset()

    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()

    const numCandidates = 3
    leaders := make(chan string, numCandidates)
    var wg sync.WaitGroup

    // è¤‡æ•°ã®å€™è£œè€…ã‚’èµ·å‹•
    for i := 0; i < numCandidates; i++ {
        wg.Add(1)
        podName := fmt.Sprintf("test-pod-%d", i)

        go func(identity string) {
            defer wg.Done()

            lock := &resourcelock.LeaseLock{
                LeaseMeta: metav1.ObjectMeta{
                    Name:      "test-lease",
                    Namespace: "default",
                },
                Client: clientset.CoordinationV1(),
                LockConfig: resourcelock.ResourceLockConfig{
                    Identity: identity,
                },
            }

            leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
                Lock:            lock,
                ReleaseOnCancel: true,
                LeaseDuration:   5 * time.Second,
                RenewDeadline:   3 * time.Second,
                RetryPeriod:     1 * time.Second,
                Callbacks: leaderelection.LeaderCallbacks{
                    OnStartedLeading: func(ctx context.Context) {
                        leaders <- identity
                        <-ctx.Done()
                    },
                    OnStoppedLeading: func() {},
                    OnNewLeader:      func(identity string) {},
                },
            })
        }(podName)
    }

    // Assert: 1ã¤ã ã‘ãŒãƒªãƒ¼ãƒ€ãƒ¼ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèª
    select {
    case leader := <-leaders:
        t.Logf("âœ… Leader elected: %s", leader)

        // çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦ä»–ã®ãƒªãƒ¼ãƒ€ãƒ¼ãŒã„ãªã„ã“ã¨ã‚’ç¢ºèª
        select {
        case duplicateLeader := <-leaders:
            t.Fatalf("âŒ Multiple leaders detected: %s", duplicateLeader)
        case <-time.After(3 * time.Second):
            t.Log("âœ… Only one leader exists")
        }
    case <-time.After(10 * time.Second):
        t.Fatal("âŒ No leader elected")
    }

    cancel()
    wg.Wait()
}
```

#### 2.3.3 ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—æ”¾æ£„ã®ãƒ†ã‚¹ãƒˆ

```go
func TestLeaderElection_GracefulShutdown(t *testing.T) {
    clientset := fake.NewSimpleClientset()

    ctx, cancel := context.WithCancel(context.Background())
    
    lock := &resourcelock.LeaseLock{
        LeaseMeta: metav1.ObjectMeta{
            Name:      "test-lease",
            Namespace: "default",
        },
        Client: clientset.CoordinationV1(),
        LockConfig: resourcelock.ResourceLockConfig{
            Identity: "test-pod-1",
        },
    }

    leadershipLost := make(chan struct{})
    leaderStarted := make(chan struct{})
    
    go func() {
        leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
            Lock:            lock,
            ReleaseOnCancel: true,  // é‡è¦: ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ™‚ã«ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã‚’æ”¾æ£„
            LeaseDuration:   5 * time.Second,
            RenewDeadline:   3 * time.Second,
            RetryPeriod:     1 * time.Second,
            Callbacks: leaderelection.LeaderCallbacks{
                OnStartedLeading: func(ctx context.Context) {
                    close(leaderStarted)
                    <-ctx.Done()
                },
                OnStoppedLeading: func() {
                    close(leadershipLost)
                },
                OnNewLeader: func(identity string) {},
            },
        })
    }()

    // ãƒªãƒ¼ãƒ€ãƒ¼ã«ãªã‚‹ã®ã‚’å¾…ã¤
    <-leaderStarted
    t.Log("Leader started")

    // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
    cancel()

    // ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ãŒæ”¾æ£„ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
    select {
    case <-leadershipLost:
        t.Log("âœ… Leadership gracefully released")
    case <-time.After(10 * time.Second):
        t.Fatal("âŒ Leadership was not released")
    }
}
```

### 2.4 ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

```bash
# ã™ã¹ã¦ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
go test -v ./...

# ç‰¹å®šã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
go test -v -run TestLeaderElection_SingleCandidate

# ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’å–å¾—
go test -v -coverprofile=coverage.out ./...
go tool cover -html=coverage.out -o coverage.html
```

### 2.5 Fake Clientã®åˆ¶é™äº‹é …

```mermaid
graph TB
    subgraph "Fake Clientã§ãƒ†ã‚¹ãƒˆå¯èƒ½"
        T1[ãƒªãƒ¼ãƒ€ãƒ¼é¸å‡ºãƒ­ã‚¸ãƒƒã‚¯]
        T2[ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®å‹•ä½œ]
        T3[ã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨­å®š]
        T4[ç«¶åˆã®åŸºæœ¬å‹•ä½œ]
    end
    
    subgraph "Fake Clientã§ãƒ†ã‚¹ãƒˆå›°é›£"
        L1[ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶]
        L2[å®Ÿéš›ã®API Serverå‹•ä½œ]
        L3[RBACæ¨©é™ã‚¨ãƒ©ãƒ¼]
        L4[Podé–“ã®å®Ÿéš›ã®ç«¶åˆ]
    end
    
    style T1 fill:#6bcb77,color:#fff
    style T2 fill:#6bcb77,color:#fff
    style T3 fill:#6bcb77,color:#fff
    style T4 fill:#6bcb77,color:#fff
    style L1 fill:#ff6b6b,color:#fff
    style L2 fill:#ff6b6b,color:#fff
    style L3 fill:#ff6b6b,color:#fff
    style L4 fill:#ff6b6b,color:#fff
```

---

## 3. çµ±åˆãƒ†ã‚¹ãƒˆ

### 3.1 æ¦‚è¦

çµ±åˆãƒ†ã‚¹ãƒˆã§ã¯ã€**Kindï¼ˆKubernetes in Dockerï¼‰**
ã‚’ä½¿ç”¨ã—ã¦ã€å®Ÿéš›ã®Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ç’°å¢ƒã§ãƒ†ã‚¹ãƒˆã‚’è¡Œã„ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "Docker Host"
        subgraph "Kind Cluster"
            API[API Server]
            ETCD[(etcd)]
            subgraph "ãƒ†ã‚¹ãƒˆå¯¾è±¡Pod"
                APP[Leader Election App]
            end
        end
        TEST[Go Test<br/>çµ±åˆãƒ†ã‚¹ãƒˆ]
    end
    
    TEST -->|kubectl / client-go| API
    APP --> API
    API --> ETCD
    
    style TEST fill:#ffd93d
    style APP fill:#4a90d9,color:#fff
```

### 3.2 Kind ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 3.2.1 Kind ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# macOS (Homebrew)
brew install kind

# Linux
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind

# Go install
go install sigs.k8s.io/kind@v0.20.0
```

#### 3.2.2 ãƒ†ã‚¹ãƒˆç”¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ä½œæˆ

```bash
# ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ä½œæˆ
kind create cluster --name leader-election-test

# ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒ“ãƒ«ãƒ‰ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ã«ãƒ­ãƒ¼ãƒ‰
docker build -t leader-election:test .
kind load docker-image leader-election:test --name leader-election-test

# ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/rbac.yaml

# ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚¿ã‚°ã«æ›´æ–°ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤
sed 's|leader-election:latest|leader-election:test|g' k8s/deployment.yaml | kubectl apply -f -

# Pod ãŒèµ·å‹•ã™ã‚‹ã¾ã§å¾…æ©Ÿ
kubectl rollout status deployment/leader-election -n leader-election-demo --timeout=120s
```

### 3.3 çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè£…

#### 3.3.1 ãƒ†ã‚¹ãƒˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°

```go
// integration_test.go
//go:build integration
// +build integration

package main

import (
    "context"
    "os"
    "path/filepath"
    "testing"
    "time"

    coordinationv1 "k8s.io/api/coordination/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/tools/clientcmd"
)

const (
    testNamespace = "leader-election-demo"
    leaseName     = "leader-election-lease"
)

// ãƒ†ã‚¹ãƒˆç”¨ã®Kubernetesã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—
func getTestClientset(t *testing.T) *kubernetes.Clientset {
    t.Helper()

    kubeconfig := os.Getenv("KUBECONFIG")
    if kubeconfig == "" {
        home, _ := os.UserHomeDir()
        kubeconfig = filepath.Join(home, ".kube", "config")
    }

    config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
    if err != nil {
        t.Fatalf("Failed to build config: %v", err)
    }

    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
        t.Fatalf("Failed to create clientset: %v", err)
    }

    return clientset
}

// Leaseã®ç¾åœ¨ã®ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å–å¾—
func getCurrentLeader(t *testing.T, clientset *kubernetes.Clientset) string {
    t.Helper()

    ctx := context.Background()
    lease, err := clientset.CoordinationV1().Leases(testNamespace).Get(
        ctx, leaseName, metav1.GetOptions{},
    )
    if err != nil {
        return ""
    }

    if lease.Spec.HolderIdentity == nil {
        return ""
    }

    return *lease.Spec.HolderIdentity
}

// ç‰¹å®šã®æ¡ä»¶ãŒæº€ãŸã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
func waitFor(t *testing.T, timeout time.Duration, condition func() bool, message string) {
    t.Helper()

    deadline := time.Now().Add(timeout)
    for time.Now().Before(deadline) {
        if condition() {
            return
        }
        time.Sleep(1 * time.Second)
    }
    t.Fatalf("Timeout waiting for: %s", message)
}
```

#### 3.3.2 ãƒªãƒ¼ãƒ€ãƒ¼å­˜åœ¨ç¢ºèªãƒ†ã‚¹ãƒˆ

```go
func TestIntegration_LeaderExists(t *testing.T) {
    clientset := getTestClientset(t)

    // ãƒªãƒ¼ãƒ€ãƒ¼ãŒé¸å‡ºã•ã‚Œã‚‹ã®ã‚’å¾…ã¤
    waitFor(t, 30*time.Second, func() bool {
        leader := getCurrentLeader(t, clientset)
        return leader != ""
    }, "leader to be elected")

    leader := getCurrentLeader(t, clientset)
    t.Logf("âœ… Current leader: %s", leader)
}
```

#### 3.3.3 ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ãƒ†ã‚¹ãƒˆ

```go
func TestIntegration_Failover(t *testing.T) {
    clientset := getTestClientset(t)
    ctx := context.Background()

    // ç¾åœ¨ã®ãƒªãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—
    originalLeader := getCurrentLeader(t, clientset)
    if originalLeader == "" {
        t.Fatal("No leader found")
    }
    t.Logf("Original leader: %s", originalLeader)

    // ãƒªãƒ¼ãƒ€ãƒ¼Podã‚’å‰Šé™¤
    err := clientset.CoreV1().Pods(testNamespace).Delete(
        ctx, originalLeader, metav1.DeleteOptions{},
    )
    if err != nil {
        t.Fatalf("Failed to delete leader pod: %v", err)
    }
    t.Log("Leader pod deleted")

    // æ–°ã—ã„ãƒªãƒ¼ãƒ€ãƒ¼ãŒé¸å‡ºã•ã‚Œã‚‹ã®ã‚’å¾…ã¤
    waitFor(t, 60*time.Second, func() bool {
        newLeader := getCurrentLeader(t, clientset)
        return newLeader != "" && newLeader != originalLeader
    }, "new leader to be elected")

    newLeader := getCurrentLeader(t, clientset)
    t.Logf("âœ… New leader elected: %s", newLeader)

    if newLeader == originalLeader {
        t.Fatal("âŒ Leader did not change")
    }
}
```

#### 3.3.4 ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ

```go
func TestIntegration_ScaleUp(t *testing.T) {
    clientset := getTestClientset(t)
    ctx := context.Background()

    // ç¾åœ¨ã®ãƒªãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—
    originalLeader := getCurrentLeader(t, clientset)
    t.Logf("Original leader: %s", originalLeader)

    // ãƒ¬ãƒ—ãƒªã‚«æ•°ã‚’å¢—ã‚„ã™
    deployment, err := clientset.AppsV1().Deployments(testNamespace).Get(
        ctx, "leader-election", metav1.GetOptions{},
    )
    if err != nil {
        t.Fatalf("Failed to get deployment: %v", err)
    }

    originalReplicas := *deployment.Spec.Replicas
    newReplicas := int32(5)
    deployment.Spec.Replicas = &newReplicas

    _, err = clientset.AppsV1().Deployments(testNamespace).Update(
        ctx, deployment, metav1.UpdateOptions{},
    )
    if err != nil {
        t.Fatalf("Failed to scale deployment: %v", err)
    }
    t.Logf("Scaled from %d to %d replicas", originalReplicas, newReplicas)

    // ã™ã¹ã¦ã®PodãŒèµ·å‹•ã™ã‚‹ã®ã‚’å¾…ã¤
    waitFor(t, 120*time.Second, func() bool {
        pods, _ := clientset.CoreV1().Pods(testNamespace).List(ctx, metav1.ListOptions{
            LabelSelector: "app=leader-election",
        })
        readyCount := 0
        for _, pod := range pods.Items {
            for _, cond := range pod.Status.Conditions {
                if cond.Type == "Ready" && cond.Status == "True" {
                    readyCount++
                }
            }
        }
        return readyCount == int(newReplicas)
    }, "all pods to be ready")

    // ãƒªãƒ¼ãƒ€ãƒ¼ãŒå¤‰ã‚ã£ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
    currentLeader := getCurrentLeader(t, clientset)
    if currentLeader != originalLeader {
        t.Logf("âš ï¸ Leader changed from %s to %s during scale up", originalLeader, currentLeader)
    } else {
        t.Log("âœ… Leader remained stable during scale up")
    }

    // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: å…ƒã®ãƒ¬ãƒ—ãƒªã‚«æ•°ã«æˆ»ã™
    deployment.Spec.Replicas = &originalReplicas
    _, _ = clientset.AppsV1().Deployments(testNamespace).Update(ctx, deployment, metav1.UpdateOptions{})
}
```

### 3.4 çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

```bash
# Kind ã‚¯ãƒ©ã‚¹ã‚¿ãŒå¿…è¦
kind create cluster --name leader-election-test

# çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼ˆãƒ“ãƒ«ãƒ‰ã‚¿ã‚°ã‚’æŒ‡å®šï¼‰
go test -v -tags=integration ./...

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«è¨­å®š
go test -v -tags=integration -timeout=10m ./...
```

### 3.5 ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

```bash
# Kindã‚¯ãƒ©ã‚¹ã‚¿ã‚’å‰Šé™¤
kind delete cluster --name leader-election-test
```

---

## 4. E2Eãƒ†ã‚¹ãƒˆï¼ˆGinkgoï¼‰

### 4.1 æ¦‚è¦

E2Eãƒ†ã‚¹ãƒˆã§ã¯ã€**Ginkgo** ã¨ **Gomega**
ã‚’ä½¿ç”¨ã—ã¦ã€BDDã‚¹ã‚¿ã‚¤ãƒ«ã®èª­ã¿ã‚„ã™ã„ãƒ†ã‚¹ãƒˆã‚’è¨˜è¿°ã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "E2Eãƒ†ã‚¹ãƒˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
        GINKGO[Ginkgo Test Suite]
        GOMEGA[Gomega Assertions]
        CLIENT[Kubernetes Client]
        
        subgraph "Kind Cluster"
            DEPLOY[Deployment]
            PODS[Pods]
            LEASE[Lease]
        end
    end
    
    GINKGO --> GOMEGA
    GINKGO --> CLIENT
    CLIENT --> DEPLOY
    CLIENT --> PODS
    CLIENT --> LEASE
    
    style GINKGO fill:#ff6b6b,color:#fff
    style GOMEGA fill:#ffd93d
```

### 4.2 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
k8s-leader-election-sample/
â”œâ”€â”€ main.go
â”œâ”€â”€ main_test.go              # ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ e2e/                      # E2Eãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ e2e_suite_test.go     # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆè¨­å®š
â”‚   â”œâ”€â”€ leader_election_test.go
â”‚   â””â”€â”€ utils.go              # ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ e2e.yaml
```

### 4.3 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Ginkgo CLI ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
go install github.com/onsi/ginkgo/v2/ginkgo@latest

# ä¾å­˜é–¢ä¿‚ã®è¿½åŠ 
go get github.com/onsi/ginkgo/v2
go get github.com/onsi/gomega

# E2Eãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p e2e
cd e2e

# ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®åˆæœŸåŒ–
ginkgo bootstrap
```

### 4.4 ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè£…

#### 4.4.1 ã‚¹ã‚¤ãƒ¼ãƒˆè¨­å®šï¼ˆe2e_suite_test.goï¼‰

```go
// e2e/e2e_suite_test.go
package e2e

import (
    "os"
    "path/filepath"
    "testing"

    . "github.com/onsi/ginkgo/v2"
    . "github.com/onsi/gomega"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/tools/clientcmd"
)

var (
    clientset *kubernetes.Clientset
    namespace = "leader-election-demo"
)

func TestE2E(t *testing.T) {
    RegisterFailHandler(Fail)
    RunSpecs(t, "Leader Election E2E Suite")
}

var _ = BeforeSuite(func() {
    By("Setting up Kubernetes client")

    kubeconfig := os.Getenv("KUBECONFIG")
    if kubeconfig == "" {
        home, err := os.UserHomeDir()
        Expect(err).NotTo(HaveOccurred())
        kubeconfig = filepath.Join(home, ".kube", "config")
    }

    config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
    Expect(err).NotTo(HaveOccurred(), "Failed to build kubeconfig")

    clientset, err = kubernetes.NewForConfig(config)
    Expect(err).NotTo(HaveOccurred(), "Failed to create clientset")

    By("Kubernetes client ready")
})

var _ = AfterSuite(func() {
    By("Cleaning up resources")
    // å¿…è¦ã«å¿œã˜ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
})
```

#### 4.4.2 ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆutils.goï¼‰

```go
// e2e/utils.go
package e2e

import (
    "context"
    "time"

    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
)

const (
    leaseName      = "leader-election-lease"
    deploymentName = "leader-election"
    appLabel       = "app=leader-election"
)

// GetCurrentLeader returns the current leader's identity
func GetCurrentLeader(ctx context.Context, cs *kubernetes.Clientset, ns string) (string, error) {
    lease, err := cs.CoordinationV1().Leases(ns).Get(ctx, leaseName, metav1.GetOptions{})
    if err != nil {
        return "", err
    }
    if lease.Spec.HolderIdentity == nil {
        return "", nil
    }
    return *lease.Spec.HolderIdentity, nil
}

// GetReadyPodCount returns the count of ready pods
func GetReadyPodCount(ctx context.Context, cs *kubernetes.Clientset, ns string) (int, error) {
    pods, err := cs.CoreV1().Pods(ns).List(ctx, metav1.ListOptions{
        LabelSelector: appLabel,
    })
    if err != nil {
        return 0, err
    }

    readyCount := 0
    for _, pod := range pods.Items {
        for _, cond := range pod.Status.Conditions {
            if cond.Type == "Ready" && cond.Status == "True" {
                readyCount++
            }
        }
    }
    return readyCount, nil
}

// DeletePod deletes a pod by name
func DeletePod(ctx context.Context, cs *kubernetes.Clientset, ns, name string) error {
    return cs.CoreV1().Pods(ns).Delete(ctx, name, metav1.DeleteOptions{})
}

// ScaleDeployment scales a deployment to the specified replicas
func ScaleDeployment(ctx context.Context, cs *kubernetes.Clientset, ns string, replicas int32) error {
    deployment, err := cs.AppsV1().Deployments(ns).Get(ctx, deploymentName, metav1.GetOptions{})
    if err != nil {
        return err
    }
    deployment.Spec.Replicas = &replicas
    _, err = cs.AppsV1().Deployments(ns).Update(ctx, deployment, metav1.UpdateOptions{})
    return err
}
```

#### 4.4.3 ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆï¼ˆleader_election_test.goï¼‰

```go
// e2e/leader_election_test.go
package e2e

import (
    "context"
    "time"

    . "github.com/onsi/ginkgo/v2"
    . "github.com/onsi/gomega"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

var _ = Describe("Leader Election", func() {
    var ctx context.Context

    BeforeEach(func() {
        ctx = context.Background()
    })

    Describe("Basic Functionality", func() {
        It("should have exactly one leader", func() {
            By("Checking if a leader exists")
            Eventually(func() string {
                leader, _ := GetCurrentLeader(ctx, clientset, namespace)
                return leader
            }, 30*time.Second, 2*time.Second).ShouldNot(BeEmpty())

            leader, err := GetCurrentLeader(ctx, clientset, namespace)
            Expect(err).NotTo(HaveOccurred())
            GinkgoWriter.Printf("Current leader: %s\n", leader)
        })

        It("should have all pods in Ready state", func() {
            By("Checking pod readiness")
            Eventually(func() int {
                count, _ := GetReadyPodCount(ctx, clientset, namespace)
                return count
            }, 60*time.Second, 5*time.Second).Should(BeNumerically(">=", 1))
        })
    })

    Describe("Failover", func() {
        It("should elect a new leader when current leader is deleted", func() {
            By("Getting the current leader")
            var originalLeader string
            Eventually(func() string {
                leader, _ := GetCurrentLeader(ctx, clientset, namespace)
                originalLeader = leader
                return leader
            }, 30*time.Second, 2*time.Second).ShouldNot(BeEmpty())

            GinkgoWriter.Printf("Original leader: %s\n", originalLeader)

            By("Deleting the leader pod")
            err := DeletePod(ctx, clientset, namespace, originalLeader)
            Expect(err).NotTo(HaveOccurred())

            By("Waiting for a new leader to be elected")
            Eventually(func() string {
                leader, _ := GetCurrentLeader(ctx, clientset, namespace)
                return leader
            }, 60*time.Second, 2*time.Second).Should(And(
                Not(BeEmpty()),
                Not(Equal(originalLeader)),
            ))

            newLeader, err := GetCurrentLeader(ctx, clientset, namespace)
            Expect(err).NotTo(HaveOccurred())
            GinkgoWriter.Printf("New leader elected: %s\n", newLeader)
        })
    })

    Describe("Scaling", func() {
        var originalReplicas int32

        BeforeEach(func() {
            deployment, err := clientset.AppsV1().Deployments(namespace).Get(
                ctx, deploymentName, metav1.GetOptions{},
            )
            Expect(err).NotTo(HaveOccurred())
            originalReplicas = *deployment.Spec.Replicas
        })

        AfterEach(func() {
            By("Restoring original replica count")
            err := ScaleDeployment(ctx, clientset, namespace, originalReplicas)
            Expect(err).NotTo(HaveOccurred())

            Eventually(func() int {
                count, _ := GetReadyPodCount(ctx, clientset, namespace)
                return count
            }, 120*time.Second, 5*time.Second).Should(Equal(int(originalReplicas)))
        })

        It("should maintain leadership during scale up", func() {
            By("Getting the current leader")
            originalLeader, err := GetCurrentLeader(ctx, clientset, namespace)
            Expect(err).NotTo(HaveOccurred())
            GinkgoWriter.Printf("Original leader: %s\n", originalLeader)

            By("Scaling up to 5 replicas")
            err = ScaleDeployment(ctx, clientset, namespace, 5)
            Expect(err).NotTo(HaveOccurred())

            By("Waiting for all pods to be ready")
            Eventually(func() int {
                count, _ := GetReadyPodCount(ctx, clientset, namespace)
                return count
            }, 120*time.Second, 5*time.Second).Should(Equal(5))

            By("Verifying leader stability")
            // ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—å¾Œã‚‚ãƒªãƒ¼ãƒ€ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            currentLeader, err := GetCurrentLeader(ctx, clientset, namespace)
            Expect(err).NotTo(HaveOccurred())
            Expect(currentLeader).NotTo(BeEmpty())
            GinkgoWriter.Printf("Leader after scale up: %s\n", currentLeader)
        })

        It("should elect a new leader after scale down to 1", func() {
            By("Scaling down to 1 replica")
            err := ScaleDeployment(ctx, clientset, namespace, 1)
            Expect(err).NotTo(HaveOccurred())

            By("Waiting for single pod to be ready")
            Eventually(func() int {
                count, _ := GetReadyPodCount(ctx, clientset, namespace)
                return count
            }, 120*time.Second, 5*time.Second).Should(Equal(1))

            By("Verifying the single pod is the leader")
            Eventually(func() string {
                leader, _ := GetCurrentLeader(ctx, clientset, namespace)
                return leader
            }, 30*time.Second, 2*time.Second).ShouldNot(BeEmpty())
        })
    })

    Describe("Lease Resource", func() {
        It("should have valid lease metadata", func() {
            By("Getting the lease resource")
            lease, err := clientset.CoordinationV1().Leases(namespace).Get(
                ctx, leaseName, metav1.GetOptions{},
            )
            Expect(err).NotTo(HaveOccurred())

            By("Verifying lease properties")
            Expect(lease.Spec.HolderIdentity).NotTo(BeNil())
            Expect(lease.Spec.LeaseDurationSeconds).NotTo(BeNil())
            Expect(*lease.Spec.LeaseDurationSeconds).To(Equal(int32(15)))

            GinkgoWriter.Printf("Lease holder: %s\n", *lease.Spec.HolderIdentity)
            GinkgoWriter.Printf("Lease duration: %d seconds\n", *lease.Spec.LeaseDurationSeconds)
        })

        It("should update renewTime periodically", func() {
            By("Getting initial lease")
            lease1, err := clientset.CoordinationV1().Leases(namespace).Get(
                ctx, leaseName, metav1.GetOptions{},
            )
            Expect(err).NotTo(HaveOccurred())
            initialRenewTime := lease1.Spec.RenewTime

            By("Waiting for lease renewal")
            time.Sleep(5 * time.Second)

            By("Getting updated lease")
            lease2, err := clientset.CoordinationV1().Leases(namespace).Get(
                ctx, leaseName, metav1.GetOptions{},
            )
            Expect(err).NotTo(HaveOccurred())
            updatedRenewTime := lease2.Spec.RenewTime

            By("Verifying renewTime has been updated")
            Expect(updatedRenewTime.Time.After(initialRenewTime.Time)).To(BeTrue(),
                "renewTime should be updated")
        })
    })
})
```

### 4.5 E2Eãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

```bash
# Kind ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
kind create cluster --name e2e-test
docker build -t leader-election:test .
kind load docker-image leader-election:test --name e2e-test

# ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’ãƒ‡ãƒ—ãƒ­ã‚¤
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/rbac.yaml
sed 's|leader-election:latest|leader-election:test|g' k8s/deployment.yaml | kubectl apply -f -
kubectl rollout status deployment/leader-election -n leader-election-demo --timeout=120s

# E2Eãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
cd e2e
ginkgo -v --timeout=10m

# ç‰¹å®šã®ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
ginkgo -v --focus="Failover"

# è©³ç´°ãªãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
ginkgo -v --json-report=report.json
```

### 4.6 Ginkgo ã®ãƒ†ã‚¹ãƒˆæ§‹é€ 

```mermaid
graph TB
    subgraph "Ginkgo ãƒ†ã‚¹ãƒˆæ§‹é€ "
        SUITE[Describe: Leader Election]
        
        SUITE --> BASIC[Describe: Basic Functionality]
        SUITE --> FAILOVER[Describe: Failover]
        SUITE --> SCALING[Describe: Scaling]
        SUITE --> LEASE[Describe: Lease Resource]
        
        BASIC --> IT1[It: should have exactly one leader]
        BASIC --> IT2[It: should have all pods Ready]
        
        FAILOVER --> IT3[It: should elect new leader<br/>when current is deleted]
        
        SCALING --> IT4[It: should maintain leadership<br/>during scale up]
        SCALING --> IT5[It: should elect new leader<br/>after scale down]
        
        LEASE --> IT6[It: should have valid metadata]
        LEASE --> IT7[It: should update renewTime]
    end
    
    style SUITE fill:#ff6b6b,color:#fff
    style BASIC fill:#ffd93d
    style FAILOVER fill:#ffd93d
    style SCALING fill:#ffd93d
    style LEASE fill:#ffd93d
```

---

## 5. GitHub Actions ã§ã®CI/CDè¨­å®š

### 5.1 ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ¦‚è¦

```mermaid
flowchart LR
    subgraph "GitHub Actions Pipeline"
        PUSH[Push / PR] --> UNIT[Unit Tests]
        UNIT --> BUILD[Docker Build]
        BUILD --> E2E[E2E Tests<br/>on Kind]
        E2E --> REPORT[Test Report]
    end
    
    style PUSH fill:#4a90d9,color:#fff
    style UNIT fill:#6bcb77,color:#fff
    style BUILD fill:#ffd93d
    style E2E fill:#ff6b6b,color:#fff
```

### 5.2 ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```yaml
# .github/workflows/unit-test.yaml
name: Unit Tests

on:
    push:
        branches: [main]
    pull_request:
        branches: [main]

jobs:
    unit-test:
        runs-on: ubuntu-latest
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Go
              uses: actions/setup-go@v5
              with:
                  go-version: "1.23"
                  cache: true

            - name: Download dependencies
              run: go mod download

            - name: Run unit tests
              run: go test -v -race -coverprofile=coverage.out ./...

            - name: Upload coverage report
              uses: codecov/codecov-action@v4
              with:
                  files: ./coverage.out
                  fail_ci_if_error: false
```

### 5.3 E2Eãƒ†ã‚¹ãƒˆç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```yaml
# .github/workflows/e2e-test.yaml
name: E2E Tests

on:
    push:
        branches: [main]
    pull_request:
        branches: [main]

env:
    KIND_CLUSTER_NAME: e2e-test
    REGISTRY: ghcr.io
    IMAGE_NAME: ${{ github.repository }}

jobs:
    e2e-test:
        runs-on: ubuntu-latest
        timeout-minutes: 30

        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Go
              uses: actions/setup-go@v5
              with:
                  go-version: "1.23"
                  cache: true

            - name: Install Ginkgo
              run: go install github.com/onsi/ginkgo/v2/ginkgo@latest

            - name: Set up Docker Buildx
              uses: docker/setup-buildx-action@v3

            - name: Build Docker image
              uses: docker/build-push-action@v5
              with:
                  context: .
                  push: false
                  load: true
                  tags: leader-election:test
                  cache-from: type=gha
                  cache-to: type=gha,mode=max

            - name: Create Kind cluster
              uses: helm/kind-action@v1
              with:
                  cluster_name: ${{ env.KIND_CLUSTER_NAME }}
                  wait: 120s

            - name: Load image to Kind
              run: |
                  kind load docker-image leader-election:test --name ${{ env.KIND_CLUSTER_NAME }}

            - name: Deploy application
              run: |
                  kubectl apply -f k8s/namespace.yaml
                  kubectl apply -f k8s/rbac.yaml
                  sed 's|leader-election:latest|leader-election:test|g' k8s/deployment.yaml | kubectl apply -f -

                  echo "Waiting for deployment to be ready..."
                  kubectl rollout status deployment/leader-election \
                    -n leader-election-demo \
                    --timeout=180s

            - name: Wait for leader election
              run: |
                  echo "Waiting for leader to be elected..."
                  for i in {1..30}; do
                    LEADER=$(kubectl get lease leader-election-lease \
                      -n leader-election-demo \
                      -o jsonpath='{.spec.holderIdentity}' 2>/dev/null || echo "")
                    if [ -n "$LEADER" ]; then
                      echo "Leader elected: $LEADER"
                      break
                    fi
                    echo "Waiting... ($i/30)"
                    sleep 2
                  done

            - name: Run E2E tests
              run: |
                  cd e2e
                  ginkgo -v --timeout=10m --json-report=report.json ./...

            - name: Upload test report
              if: always()
              uses: actions/upload-artifact@v4
              with:
                  name: e2e-test-report
                  path: e2e/report.json

            - name: Collect logs on failure
              if: failure()
              run: |
                  echo "=== Pod Status ==="
                  kubectl get pods -n leader-election-demo -o wide

                  echo "=== Pod Logs ==="
                  kubectl logs -n leader-election-demo -l app=leader-election --tail=100

                  echo "=== Lease Status ==="
                  kubectl get lease -n leader-election-demo -o yaml

                  echo "=== Events ==="
                  kubectl get events -n leader-election-demo --sort-by='.lastTimestamp'

            - name: Delete Kind cluster
              if: always()
              run: |
                  kind delete cluster --name ${{ env.KIND_CLUSTER_NAME }}
```

### 5.4 çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆå…¨ãƒ†ã‚¹ãƒˆï¼‰

```yaml
# .github/workflows/ci.yaml
name: CI

on:
    push:
        branches: [main]
    pull_request:
        branches: [main]

jobs:
    unit-tests:
        runs-on: ubuntu-latest
        steps:
            - uses: actions/checkout@v4
            - uses: actions/setup-go@v5
              with:
                  go-version: "1.23"
                  cache: true
            - run: go test -v -race ./...

    e2e-tests:
        needs: unit-tests
        runs-on: ubuntu-latest
        timeout-minutes: 30
        steps:
            - uses: actions/checkout@v4

            - uses: actions/setup-go@v5
              with:
                  go-version: "1.23"
                  cache: true

            - run: go install github.com/onsi/ginkgo/v2/ginkgo@latest

            - name: Build image
              run: docker build -t leader-election:test .

            - name: Create Kind cluster
              uses: helm/kind-action@v1
              with:
                  cluster_name: e2e-test

            - name: Setup and test
              run: |
                  kind load docker-image leader-election:test --name e2e-test
                  kubectl apply -f k8s/
                  sed -i 's|leader-election:latest|leader-election:test|g' k8s/deployment.yaml
                  kubectl apply -f k8s/deployment.yaml
                  kubectl rollout status deployment/leader-election -n leader-election-demo --timeout=180s
                  cd e2e && ginkgo -v --timeout=10m
```

### 5.5 ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ã®æœ€é©åŒ–

```mermaid
gantt
    title CI Pipeline ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
    dateFormat mm:ss
    axisFormat %M:%S
    
    section Unit Tests
    Checkout & Setup    :a1, 00:00, 30s
    Run Tests           :a2, after a1, 15s
    
    section E2E Tests
    Checkout & Setup    :b1, 00:00, 30s
    Build Image         :b2, after b1, 60s
    Create Kind         :b3, after b2, 90s
    Deploy App          :b4, after b3, 60s
    Run E2E Tests       :b5, after b4, 180s
```

---

## 6. ãƒ†ã‚¹ãƒˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 6.1 ãƒ†ã‚¹ãƒˆã®å‘½åè¦å‰‡

```go
// âœ… Good: ä½•ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã„ã‚‹ã‹æ˜ç¢º
func TestLeaderElection_WhenLeaderDeleted_NewLeaderIsElected(t *testing.T)
func TestLeaderElection_WithMultipleCandidates_OnlyOneBecomesLeader(t *testing.T)

// âŒ Bad: æ›–æ˜§
func TestLeader(t *testing.T)
func Test1(t *testing.T)
```

### 6.2 ãƒ†ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š

```go
// Leader Electionã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
const (
    // ãƒ†ã‚¹ãƒˆç”¨ã®çŸ­ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    testLeaseDuration = 5 * time.Second
    testRenewDeadline = 3 * time.Second
    testRetryPeriod   = 1 * time.Second
    
    // ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ã®æœ€å¤§å¾…ã¡æ™‚é–“
    // = LeaseDuration + RenewDeadline + ãƒãƒƒãƒ•ã‚¡
    failoverTimeout = 15 * time.Second
)
```

### 6.3 ä¸¦åˆ—ãƒ†ã‚¹ãƒˆã®æ³¨æ„ç‚¹

```go
// âš ï¸ ä¸¦åˆ—å®Ÿè¡Œæ™‚ã¯åŒã˜Leaseã‚’ä½¿ã‚ãªã„
func TestLeaderElection_Parallel(t *testing.T) {
    t.Parallel() // ã“ã‚Œã‚’ä½¿ã†å ´åˆã¯æ³¨æ„
    
    // å„ãƒ†ã‚¹ãƒˆã§ä¸€æ„ã®Leaseåã‚’ä½¿ç”¨
    leaseName := fmt.Sprintf("test-lease-%s", t.Name())
    // ...
}
```

### 6.4 ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

```go
func TestWithCleanup(t *testing.T) {
    // ãƒ†ã‚¹ãƒˆç”¨ã®Leaseã‚’ä½œæˆ
    leaseName := "test-lease"
    
    // ãƒ†ã‚¹ãƒˆçµ‚äº†æ™‚ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    t.Cleanup(func() {
        ctx := context.Background()
        _ = clientset.CoordinationV1().Leases(namespace).Delete(
            ctx, leaseName, metav1.DeleteOptions{},
        )
    })
    
    // ãƒ†ã‚¹ãƒˆæœ¬ä½“
}
```

### 6.5 ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å‡ºåŠ›

```go
// Ginkgo ã§ã®è©³ç´°ãƒ­ã‚°
By("Checking leader status")
GinkgoWriter.Printf("Current leader: %s\n", leader)
GinkgoWriter.Printf("Lease renewTime: %v\n", lease.Spec.RenewTime)

// æ¨™æº–ãƒ†ã‚¹ãƒˆã§ã®è©³ç´°ãƒ­ã‚°
t.Logf("Current leader: %s", leader)
if testing.Verbose() {
    t.Logf("Detailed lease info: %+v", lease.Spec)
}
```

### 6.6 ãƒ†ã‚¹ãƒˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ãƒ†ã‚¹ãƒˆé …ç›®            | ãƒ¦ãƒ‹ãƒƒãƒˆ | çµ±åˆ | E2E |
| --------------------- | :------: | :--: | :-: |
| ãƒªãƒ¼ãƒ€ãƒ¼é¸å‡ºæˆåŠŸ      |    âœ…    |  âœ…  | âœ…  |
| è¤‡æ•°å€™è£œè€…ã®ç«¶åˆ      |    âœ…    |  âœ…  | âœ…  |
| ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼      |    âš ï¸    |  âœ…  | âœ…  |
| Graceful Shutdown     |    âœ…    |  âœ…  | âœ…  |
| ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—/ãƒ€ã‚¦ãƒ³ |    âŒ    |  âœ…  | âœ…  |
| ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³      |    âŒ    |  âš ï¸  | âœ…  |
| RBACæ¨©é™ã‚¨ãƒ©ãƒ¼        |    âŒ    |  âœ…  | âœ…  |
| Leaseãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼   |    âš ï¸    |  âœ…  | âœ…  |

**å‡¡ä¾‹**: âœ… é©åˆ‡ / âš ï¸ é™å®šçš„ / âŒ ä¸é©åˆ‡

---

## å‚è€ƒè³‡æ–™

- [client-go testing package documentation](https://pkg.go.dev/k8s.io/client-go/testing)
- [Ginkgo Testing Framework](https://onsi.github.io/ginkgo/)
- [Kind - Kubernetes in Docker](https://kind.sigs.k8s.io/)
- [Kubernetes E2E Testing Best Practices](https://kubernetes.io/docs/contribute/testing/)
